# TODO: To-Be-Named

## TODO: Computer vision

### Feature importances for computer vision

- LIME can be used to shade the image based on importance

## [Ethics](https://github.com/UBC-CS/cpsc330/blob/master/lectures/22_ethics.ipynb)

Problems include ethics, bias, fairness, AI safety, privacy, etc.

### Bias

_**How could this sort of bias affect peoples' lives negatively?**_
TODO

_**Where does bias come from?**_

- Data (if data only has certain groups in certain situations)
- Labels (if they were generated by humans, or not)
- Learning method (this is harder to get at)

### AI saftey and adversarial examples

TODO:
- Whitebox software attack
- Whitebox physical attack

### Fake news and deepfakes

### Environmental impact

Current methods require a lot of data, time to train, many training runs to do hyperparameter optimization

### Crime machine learning

Predicting whether someone is a criminal based on their face

- Your prediction algorithm is only as good (or bad) as your training data
- Sources of bias:
  - Wearing a white shirt and jacket vs. other clothes
  - Facial expressions (smiling vs. frowning)
  - Cropping, lighting
  - Biased criminal justice system (e.g., tends to convict less attractive faces)
  - How can our algorithm be "better" (less biased) than humans if humans labeled the data?

TODO: Consider moving this to analysis
_**Suggested strategies: experimental set-up problems**_

- Are my results too good to be true?
- Use baselines like DummyClassifier and DummyRegressor.
- Look at feature importances.
- Manually look at some of the correct/incorrect predictions (or very low/high error for regression).
- Try making changes or perturbations (e.g. different train/validation folds) and check if your results are robust.
- When you are done, think carefully about your confidence (or credence, see lecture 21) regarding any claims you make.

_**Suggested strategies: ethical/fairness issues**_

- Bias usually comes from the data, not the algorithm. Think carefully about how the training data were collected.
- Familiarize yourself with how your model will be used.
- Ask yourself who might be affected by this model in deployment.